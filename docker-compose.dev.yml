services:
  # Redis Service for WebSocket functionality
  redis:
    image: redis:7-alpine
    container_name: redis-dev
    restart: always
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # SonarQube
  sonarqube:
    build: 
      context: ./devops/sonarqube
      dockerfile: Dockerfile.dev
    container_name: sonarqube-dev
    restart: always
    ports:
      - "9000:9000"
    env_file:
      - .env
    environment:
      - SONAR_USER=${SONAR_USER}
      - SONAR_PWD=${SONAR_PWD}
    volumes:
      - sonar-token-data:/token
      - ./devops/sonarqube/init-sonar:/docker-entrypoint-init.d/init-sonar
    command: ["/bin/bash", "/docker-entrypoint-init.d/init-sonar"]
    healthcheck:
      test: ["CMD-SHELL", "curl -u ${SONAR_USER}:${SONAR_PWD} -s http://localhost:9000/api/system/health | grep -q GREEN || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s


  # ClamAV Service for virus scanning
  clamav:
    image: clamav/clamav:latest
    platform: linux/amd64
    container_name: clamav-dev
    restart: always
    ports:
      - "3310:3310"
    volumes:
      - clamav_data:/var/lib/clamav
    healthcheck:
      test: ["CMD", "clamdscan", "--version"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Django Backend (Development)
  backend:
    build: 
      context: ./backend
      dockerfile: Dockerfile.dev
    container_name: backend-dev
    restart: always
    ports:
      - "8000:8000"
    env_file:
      - .env
    environment:
      - DB_HOST=host.docker.internal
      - DEBUG=True
      - SONAR_USER=${SONAR_USER}
      - SONAR_PWD=${SONAR_PWD}
      - OTEL_SERVICE_NAME=django-backend
      - OTEL_TRACES_SAMPLER=always_on
      - KAFKA_BROKER=${KAFKA_BROKER}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      redis:
        condition: service_healthy
      clamav:
        condition: service_healthy
      sonarqube:
        condition: service_healthy
    volumes:
      - ./backend:/app
      - sonar-token-data:/token
    command: ["bash", "-lc", "python manage.py runserver 0.0.0.0:8000"]
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=django"    

  # Next.js Frontend (Development)
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.dev
    container_name: frontend-dev
    restart: always
    ports:
      - "3000:3000"
    env_file:
      - .env
    volumes:
      - ./frontend:/app
      - frontend_node_modules:/app/node_modules
      - frontend_next:/app/.next
      - sonar-token-data:/token
    environment:
      - WATCHPACK_POLLING=true
      - CHOKIDAR_USEPOLLING=true
      - SONAR_USER=${SONAR_USER}
      - SONAR_PWD=${SONAR_PWD}
      - KAFKA_BROKER=${KAFKA_BROKER}
    depends_on:
      - backend
      - sonarqube
    command: ["/bin/bash", "/app/entrypoint-dev"]
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=nextjs"    

  # Nginx Reverse Proxy
  nginx:
    image: nginx:latest
    container_name: nginx-dev
    restart: always
    ports:
      - "80:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - backend
      - frontend

  # Celery Worker for background tasks
  celery-worker:
    build:
      context: ./backend
      dockerfile: Dockerfile.dev
    container_name: celery-worker-dev
    restart: always
    env_file:
      - .env
    environment:
      - DB_HOST=host.docker.internal
      - DEBUG=True
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      redis:
        condition: service_healthy
      clamav:
        condition: service_healthy
    volumes:
      - ./backend:/app
    command: celery -A backend worker --loglevel=info

  # Ngrok tunnel service
  # ngrok:
  #   image: ngrok/ngrok:latest
  #   container_name: ngrok-dev
  #   restart: always
  #   env_file:
  #     - .env
  #   environment:
  #     - NGROK_AUTHTOKEN=${NGROK_AUTHTOKEN}
  #   volumes:
  #     - ./ngrok/ngrok.yml:/etc/ngrok.yml:ro
  #   command: start --all --config /etc/ngrok.yml
  #   depends_on:
  #     - backend

  prometheus:
    image: prom/prometheus
    volumes:
      - ./devops/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
    depends_on:
      - backend
      - frontend

  loki:
    image: grafana/loki:2.9.0
    ports:
      - "3100:3100"

  grafana:
    image: grafana/grafana
    ports:
      - "3001:3000"
    depends_on:
      - backend
      - frontend
      - prometheus

  jaeger:
    image: jaegertracing/all-in-one:1.57
    restart: always
    ports:
      - "16686:16686"   # Jaeger UI
      - "6831:6831/udp" # Agent UDP
    depends_on:
      - backend
      - frontend


  kafka:
    image: confluentinc/cp-kafka:7.6.0
    restart: always
    ports:
      - "9092:9092"      # INTERNAL listener (for containers)
      - "29092:29092"    # EXTERNAL listener (for host/CI)
      - "1234:1234"     # Confluent Metrics endpoint
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka:29093"
      KAFKA_LISTENERS: INTERNAL://0.0.0.0:9092,EXTERNAL://0.0.0.0:29092,CONTROLLER://0.0.0.0:29093
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:9092,EXTERNAL://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      # Topic management - auto-creation disabled (topics must be pre-defined)
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
      CLUSTER_ID: "MW-7IGsmQbmFkrf2x7Ho5A"
      # Retention policy (7 days default)
      KAFKA_LOG_RETENTION_HOURS: 168
      
      # Security Configuration (PLAINTEXT for dev, see KAFKA_SECURITY.md for production)
      # Current: PLAINTEXT (no authentication/encryption) - OK for local development only
      # Production: Use SASL_SSL with SCRAM-SHA-512 authentication and SSL encryption
      
      # === PRODUCTION SECURITY SETTINGS (uncomment and configure for production) ===
      # 
      # 1. Update listener security protocol map to use SASL_SSL:
      # KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:SASL_SSL,EXTERNAL:SASL_SSL,CONTROLLER:PLAINTEXT
      #
      # 2. Enable SASL mechanism:
      # KAFKA_SASL_ENABLED_MECHANISMS: SCRAM-SHA-512
      # KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: SCRAM-SHA-512
      #
      # 3. Configure JAAS (Java Authentication and Authorization Service):
      # KAFKA_OPTS: >-
      #   -Djava.security.auth.login.config=/etc/kafka/kafka_server_jaas.conf
      #   -javaagent:/opt/jmx_exporter/jmx_prometheus_javaagent.jar=9999:/config/kafka.yml
      # Mount JAAS config file in volumes: ./devops/kafka/kafka_server_jaas.conf:/etc/kafka/kafka_server_jaas.conf:ro
      #
      # 4. SSL/TLS Configuration (generate certificates first):
      # KAFKA_SSL_KEYSTORE_LOCATION: /var/ssl/private/kafka.server.keystore.jks
      # KAFKA_SSL_KEYSTORE_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      # KAFKA_SSL_KEY_PASSWORD: ${KAFKA_SSL_KEY_PASSWORD}
      # KAFKA_SSL_TRUSTSTORE_LOCATION: /var/ssl/private/kafka.server.truststore.jks
      # KAFKA_SSL_TRUSTSTORE_PASSWORD: ${KAFKA_SSL_TRUSTSTORE_PASSWORD}
      # KAFKA_SSL_CLIENT_AUTH: required
      # KAFKA_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: https
      # Mount SSL certificates in volumes: ./devops/kafka/ssl:/var/ssl/private:ro
      #
      # 5. Access Control Lists (ACLs) - enable for fine-grained authorization:
      # KAFKA_AUTHORIZER_CLASS_NAME: kafka.security.authorizer.AclAuthorizer
      # KAFKA_SUPER_USERS: User:admin
      #
      # See KAFKA_SECURITY.md for detailed security setup instructions
    volumes:
      - kafka_data:/var/lib/kafka/data   
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "9092"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 20s

      
  kafka-exporter:
    image: danielqsj/kafka-exporter
    container_name: kafka-exporter
    restart: always
    ports:
      - "9308:9308"
    command:
      - "--kafka.server=kafka:9092"   
    depends_on:
      - kafka

  kafka-ui:
    # Kafka UI for cluster management and monitoring
    # Configured for KRaft mode (no Zookeeper dependency)
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    restart: always
    ports:
      - "8081:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      # Note: Zookeeper not needed in KRaft mode - metadata managed by Kafka controllers
      # KAFKA_CLUSTERS_0_ZOOKEEPER removed - using bootstrap servers only
    depends_on:
      kafka:
        condition: service_healthy

  topic-init:
    build:
      context: ./devops/topic-init
      dockerfile: Dockerfile.dev
    container_name: topic-init
    restart: always
    entrypoint: ["/bin/bash", "/usr/local/bin/entrypoint-dev"]
    depends_on:
      kafka:
        condition: service_healthy


  kcat:
    image: edenhill/kcat:1.7.1
    container_name: kcat
    entrypoint: ["sh", "-c", "sleep 999999"]

  # InfluxDB for K6 metrics
  influxdb:
    image: influxdb:2.7-alpine
    container_name: influxdb-k6
    restart: always
    ports:
      - "8086:8086"
    env_file:
      - .env
    environment:
      - DOCKER_INFLUXDB_INIT_MODE=setup
      - DOCKER_INFLUXDB_INIT_USERNAME=${INFLUXDB_USER:-}
      - DOCKER_INFLUXDB_INIT_PASSWORD=${INFLUXDB_PASSWORD:-}
      - DOCKER_INFLUXDB_INIT_ORG=${INFLUXDB_ORG:-}
      - DOCKER_INFLUXDB_INIT_BUCKET=${INFLUXDB_BUCKET:-}
      # IMPORTANT: Set INFLUXDB_TOKEN in .env file before first run
      # If not set, InfluxDB will generate a token automatically on first startup
      # You can retrieve it later or create a new one using: docker exec influxdb-k6 influx auth create --org k6 --all-access
      - DOCKER_INFLUXDB_INIT_ADMIN_TOKEN=${INFLUXDB_TOKEN:-}
    volumes:
      - influxdb_k6_data:/var/lib/influxdb2
      - influxdb_k6_config:/etc/influxdb2
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8086/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    # Note: depends_on does not wait for service readiness
    # Services depending on InfluxDB should check health or add retry logic

  # K6 Load Testing Service
  # Run on-demand using: docker compose run --rm k6 run /scripts/scenarios/smoke-test.js
  # Or use the Python scripts: python k6/run_smoke_test.py
  k6:
    build:
      context: ./k6
      dockerfile: Dockerfile.k6
    image: k6-influxdb:latest
    container_name: k6-runner
    # Don't start automatically - run on-demand
    profiles:
      - k6
    env_file:
      - .env
    environment:
      # Test configuration
      - K6_BASE_URL=${K6_BASE_URL:-http://backend-dev:8000}
      - K6_FRONTEND_URL=${K6_FRONTEND_URL:-http://frontend-dev:3000}
      - K6_TEST_USER_EMAIL=${K6_TEST_USER_EMAIL:-}
      - K6_TEST_USER_PASSWORD=${K6_TEST_USER_PASSWORD:-}
      # InfluxDB output configuration (if using custom image)
      - K6_INFLUXDB_ADDR=${INFLUXDB_URL:-http://influxdb-k6:8086}
      - K6_INFLUXDB_ORGANIZATION=${INFLUXDB_ORG:-}
      - K6_INFLUXDB_BUCKET=${INFLUXDB_BUCKET:-}
      - K6_INFLUXDB_TOKEN=${INFLUXDB_TOKEN:-}
    volumes:
      # Mount scripts directory as read-only
      - ./k6/scripts:/scripts:ro
    # Default command (can be overridden when running)
    command: ["run", "--help"]
    depends_on:
      influxdb:
        condition: service_healthy
    # Note: Service uses default docker-compose network to communicate with other services
    # Services are accessible by their container names (e.g., backend-dev, frontend-dev, influxdb-k6)

volumes:
  clamav_data: 
  sonar-token-data:
  frontend_node_modules:
  frontend_next:
  kafka_data:
  influxdb_k6_data:
  influxdb_k6_config:
  elasticsearch_data:
